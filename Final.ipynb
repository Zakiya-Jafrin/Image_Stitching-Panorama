{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Import the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Image files\n",
    "reading all the images from the dirctory and defining the directory to save the results. A directory called result is created to save the generated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('../project/project_images')\n",
    "path_result = os.path.abspath('../project/results')\n",
    "\n",
    "img_box = path + '/Boxes.png'\n",
    "img_1 = path + '/Rainier1.png'\n",
    "img_2 = path + '/Rainier2.png'\n",
    "img_3 = path + '/Rainier3.png'\n",
    "img_4 = path + '/Rainier4.png'\n",
    "img_5 = path + '/Rainier5.png'\n",
    "img_6 = path + '/Rainier6.png'\n",
    "\n",
    "img_own_1 = path + '/1.jpg'\n",
    "img_own_2 = path + '/2.jpg'\n",
    "img_own_3 = path + '/3.jpg'\n",
    "\n",
    "img_extra_11 = path + '/Hanging1.png'\n",
    "img_extra_12 = path + '/Hanging2.png'\n",
    "\n",
    "img_extra_31 = path + '/ND1.png'\n",
    "img_extra_32 = path + '/ND2.png'\n",
    "\n",
    "imgBox = cv2.imread(img_box)\n",
    "img1 = cv2.imread(img_1)\n",
    "img2 = cv2.imread(img_2)\n",
    "img3 = cv2.imread(img_3)\n",
    "img4 = cv2.imread(img_4)\n",
    "img5 = cv2.imread(img_5)\n",
    "img6 = cv2.imread(img_6)\n",
    "\n",
    "own1 = cv2.imread(img_own_1)\n",
    "own2 = cv2.imread(img_own_2)\n",
    "own3 = cv2.imread(img_own_3)\n",
    "\n",
    "imgex11 = cv2.imread(img_extra_11)\n",
    "imgex12 = cv2.imread(img_extra_12)\n",
    "\n",
    "imgex31 = cv2.imread(img_extra_31)\n",
    "imgex32 = cv2.imread(img_extra_32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the HarrisCorner\n",
    "imeplement Harris corner detection algorithm to get the interst points and save the three images into the result folder. Non maximum suppression is also applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptiveNonMaxSup(image, keypoints, num):\n",
    "    keys_with_radius =[]\n",
    "    final_result =[]\n",
    "    for i in range(len(keypoints)):\n",
    "        key1 = keypoints[i].pt\n",
    "        lowest_radius = 9999999999\n",
    "        for j in range(len(keypoints)):\n",
    "            if(i==j):\n",
    "                continue\n",
    "            key2 = keypoints[j].pt\n",
    "            radius = math.sqrt((key1[0]-key2[0])**2 + (key1[1]-key2[1])**2)\n",
    "            if(radius < lowest_radius):\n",
    "                lowest_radius = radius\n",
    "        keys_with_radius.append([key1[0], key1[1], lowest_radius])\n",
    "        \n",
    "    keys_with_radius = sorted(keys_with_radius, key = lambda x :x[2], reverse = True)[:num]\n",
    "    for key in keys_with_radius:\n",
    "        final_result.append(cv2.KeyPoint(key[0], key[1], key[2]))\n",
    "#     outImage = cv2.drawKeypoints(image, final_result,image)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonMaxSup(harris):\n",
    "    harris = np.pad(harris, 1, mode='constant')\n",
    "    rows, cols = harris.shape\n",
    "    \n",
    "    for i in range(1, rows-1, 1):\n",
    "        for j in range(1, cols-1, 1):\n",
    "            patch = harris[i-1: i+3, j-1:j+3]\n",
    "            if(harris[i, j] != patch.max()):\n",
    "                harris[i,j ]=0\n",
    "    harris = harris[1: rows, 1:cols]\n",
    "    return harris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Corner(img, THRESHOLD):\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "    image *= 1./255\n",
    "    \n",
    "    Ix = cv2.Sobel(image, -1,1,0,ksize=3)\n",
    "    Iy = cv2.Sobel(image, -1,0,1,ksize=3)\n",
    "    \n",
    "    Ix2 = Ix**2\n",
    "    Iy2 = Iy**2\n",
    "    Ixy = Ix *Iy\n",
    "    \n",
    "    Ix2 = cv2.GaussianBlur(Ix2, (3,3), 0)\n",
    "    Iy2 = cv2.GaussianBlur(Iy2, (3,3), 0)\n",
    "    Ixy = cv2.GaussianBlur(Ixy, (3,3), 0)\n",
    "    \n",
    "    determinant = Ix2*Iy2 - Ixy*Ixy\n",
    "    trace= Ix2+Iy2\n",
    "    response = np.divide(determinant,trace)\n",
    "    response[np.isnan(response)]=0\n",
    "    magnitude = cv2.sqrt(Ix**2+Iy**2)\n",
    "    orientations=np.arctan2(Iy, Ix)\n",
    "    \n",
    "#     response = nonMaxSup(respons)\n",
    "    keypoints = np.argwhere(response> THRESHOLD)\n",
    "    keypoints = [cv2.KeyPoint(x[1],x[0], response[x[0],x[1]])for x in keypoints]\n",
    "    outImage = cv2.drawKeypoints(img, keypoints,img)\n",
    "    \n",
    "#     keyp = adaptiveNonMaxSup(image, response, 4)\n",
    "    \n",
    "    return keypoints, magnitude, np.rad2deg(orientations), outImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris corner Detection (Answer of Project 1a, 1b, and 1c)\n",
    "Call the harris function to show the image corners and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp, mg, th, keypoint_box = Corner(imgBox, 2)\n",
    "keypoints1, image1_magnitude, image1_orientations, keypoint_image1 = Corner(img1,.5)\n",
    "keypoints2, image2_magnitude, image2_orientations, keypoint_image2 = Corner(img2,.5)\n",
    "\n",
    "cv2.imshow(\"keypoint_box\", keypoint_box)\n",
    "cv2.imshow(\"keypoint_image1\", keypoint_image1)\n",
    "cv2.imshow(\"keypoint_image2\", keypoint_image2)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite(path_result+'/1a'+'.png',keypoint_box.astype(np.uint8))\n",
    "cv2.imwrite(path_result+'/1b'+'.png',keypoint_image1.astype(np.uint8))\n",
    "cv2.imwrite(path_result+'/1c'+'.png',keypoint_image2.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit Keypoint detect Using new detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SiftKeypointDetector as newDetect\n",
    "\n",
    "o, image11_magnitude, image11_orientations, keypoint_image11 = Corner(imgex11, .4) #.5, 1.1\n",
    "p, image12_magnitude, image12_orientations, keypoint_image12 = Corner(imgex12,.4)\n",
    "\n",
    "keypoints11 = newDetect.new_impl(imgex11)\n",
    "keypoints12 = newDetect.new_impl(imgex12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SiftKeypointDetector as newDetect\n",
    "\n",
    "o, image31_magnitude, image31_orientations, keypoint_image31 = Corner(imgex31, .5) #.5, 1.1\n",
    "p, image32_magnitude, image32_orientations, keypoint_image32 = Corner(imgex32,1.1)\n",
    "\n",
    "keypoints31 = newDetect.new_impl(imgex31)\n",
    "keypoints32 = newDetect.new_impl(imgex32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Descriptor\n",
    "get the dominant Angle in the the image patch, 360 degree angles devided into 10 degree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDominantAngle(patch, patch_magnitude):\n",
    "    angles = np.zeros(36)\n",
    "    for row in range(len(patch)):\n",
    "        for col in range(len(patch[0])):\n",
    "            cell_angle = patch[row, col]\n",
    "            cell_magnitude = patch_magnitude[row, col]       \n",
    "            index = int(cell_angle/10)\n",
    "            angles[index-1] += cell_magnitude\n",
    "    return angles.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation and Contrast invariance for Extra Credits, \n",
    "\n",
    "For number 3 extra credit, rotation imvarinace bins ae created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotationInvariance(patch, patch_magnitude):\n",
    "    angles = np.zeros(18)\n",
    "    for row in range(len(patch)):\n",
    "        for col in range(len(patch[0])):\n",
    "            cell_angle = patch[row, col]\n",
    "            cell_magnitude = patch_magnitude[row, col]       \n",
    "            index = int(cell_angle/20)\n",
    "            angles[index-1] += cell_magnitude\n",
    "    return angles.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Vector\n",
    "crteae 1,128 vector to describe the features of wach keypoint. For now we are ignoring the edges of the imaages. put the orientations as per their magnitudes in corresponding bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescriptor(interest_point, image_magnitude, image_orientations):\n",
    "    x, y= interest_point.pt\n",
    "    top = int(y-8)\n",
    "    left = int(x-8)\n",
    "    \n",
    "    patch = image_orientations[top:top+16, left:left+16]\n",
    "#     print(patch.shape)\n",
    "    patch_magnitude = image_magnitude[top:top+16, left:left+16]\n",
    "    dominant_angle = getDominantAngle(patch, patch_magnitude)\n",
    "    patch = patch - dominant_angle\n",
    "    \n",
    "    feature_vectors = np.zeros(128)\n",
    "    index =0\n",
    "    \n",
    "    for i in range (0,16,4):\n",
    "        for j in range(0,16,4):\n",
    "            subpatch = patch[i:i+4, j:j+4]\n",
    "            bin_1 = ((subpatch>= -180)& (subpatch<-135)).sum()\n",
    "            bin_2 = ((subpatch>= -135)& (subpatch<-90)).sum()\n",
    "            bin_3 = ((subpatch>= -90)& (subpatch<-45)).sum()\n",
    "            bin_4 = ((subpatch>= -45)& (subpatch<0)).sum()\n",
    "            bin_5 = ((subpatch>= 0)& (subpatch<45)).sum()\n",
    "            bin_6 = ((subpatch>= 45)& (subpatch<90)).sum()\n",
    "            bin_7 = ((subpatch>= 90)& (subpatch< 135)).sum()\n",
    "            bin_8 = ((subpatch>= 135)& (subpatch<180)).sum()\n",
    "            feature_vectors[index:index+8] = bin_1,bin_2,bin_3,bin_4,bin_5,bin_6,bin_7,bin_8\n",
    "            \n",
    "            index +=8\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescriptorForImage(keypoints, magnitudes, oreintations):   \n",
    "    image_descriptor=[]\n",
    "    for i in range(len(keypoints)):\n",
    "        descriptor = getDescriptor(keypoints[i], magnitudes, oreintations)\n",
    "        image_descriptor.append(descriptor)\n",
    "    return image_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistance(feature_vector1, feature_vector2):\n",
    "    return np.sum((feature_vector1-feature_vector2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD test \n",
    "Get the sum of squared difference of teh descriptor between to images so that we can get a close estimation of correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSD(image_descriptor1, image_descriptor2):\n",
    "    ssd_values =[]\n",
    "    for i in range(len(image_descriptor1)):\n",
    "        desc1 = image_descriptor1[i]\n",
    "        lowest = 999999999999\n",
    "        lowest_index = -1\n",
    "        for j in range(len(image_descriptor2)):\n",
    "            desc2 = image_descriptor2[j]\n",
    "            distance = getDistance(desc1, desc2)\n",
    "            if(distance<lowest):\n",
    "                lowest = distance\n",
    "                lowest_index =j\n",
    "        ssd_values.append([lowest, lowest_index])\n",
    "    return ssd_values\n",
    "\n",
    "def getSSD2(image_descriptor1, image_descriptor2, ssd_values):\n",
    "    ssd_values2 =[]\n",
    "    for i in range(len(image_descriptor1)):\n",
    "        desc1 = image_descriptor1[i]\n",
    "        lowest = 999999999999\n",
    "        lowest_index = -1\n",
    "        for j in range(len(image_descriptor2)):\n",
    "            desc2 = image_descriptor2[j]\n",
    "            distance = getDistance(desc1, desc2)\n",
    "            if(distance<lowest and distance != ssd_values[i][0]):\n",
    "                lowest = distance\n",
    "                lowest_index =j\n",
    "        ssd_values2.append([lowest, lowest_index])\n",
    "    return ssd_values2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio Test\n",
    "put a threshold value to pcik up the good featured of keypoints between two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratioTest(lowest_ssd, lowest_ssd2, ratio):\n",
    "    ratio_values=[]\n",
    "    for i in range(len(lowest_ssd)):\n",
    "        ssd1 = lowest_ssd[i][0]\n",
    "        ssd2 = lowest_ssd2[i][0]       \n",
    "        if(ssd1/ssd2 <= ratio):\n",
    "            ratio_values.append([ratio, i, lowest_ssd[i][1]])\n",
    "    return ratio_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast Handler \n",
    "Use SIFT implementation of Contrast handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastHandler(desc):\n",
    "    normalized_list =[]\n",
    "    for d in desc:\n",
    "        d = d/(d.sum()+.000000001)\n",
    "        d = d.clip(0,0.2)\n",
    "        d = d/d.sum()\n",
    "        normalized_list.append(d)\n",
    "    return normalized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor procedure \n",
    "all the descritors are called sequentially and processed in this block since the process requires so many processes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDescriptor(img1, kp1, mag1, orien1,img2, kp2, mag2, orien2, RATIO):\n",
    "    desc1 = getDescriptorForImage(kp1, mag1, orien1)\n",
    "    desc2 = getDescriptorForImage(kp2, mag2, orien2)\n",
    "    image1_desc = contrastHandler(desc1)\n",
    "    image2_desc = contrastHandler(desc2)\n",
    "    ssd_values = getSSD(image1_desc, image2_desc)\n",
    "    ssd_second_values = getSSD2(image1_desc, image2_desc, ssd_values)\n",
    "    ratio_values = ratioTest(ssd_values, ssd_second_values, RATIO)\n",
    "    matching_points_ratio =[]\n",
    "    for i in range(len(ratio_values)):\n",
    "        matching_points_ratio.append(cv2.DMatch(ratio_values[i][1], ratio_values[i][2], ratio_values[i][0]))\n",
    "\n",
    "    image3 = cv2.drawMatches(img1, kp1, img2, kp2, matching_points_ratio, None, flags=2)\n",
    "    cv2.imshow(\"Matching_points\", image3)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return kp1, kp2 , matching_points_ratio, image3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Descriptor to get the Matches(Answer 2)\n",
    "Save the result of the matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rainer Image //.5 and .7, .7,400\n",
    "key1, key2, good, matched_keypoints =runDescriptor(img1, keypoints1, image1_magnitude, image1_orientations,img2, keypoints2, image2_magnitude, image2_orientations, .7)\n",
    "cv2.imwrite(path_result+'/2'+'.png',matched_keypoints.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ THE IMAGES AGAIN\n",
    "--this is done mainly for debugging purpose otherwise it might produce some distoted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('../project/project_images')\n",
    "path_result = os.path.abspath('../project/results')\n",
    "\n",
    "img_1 = path + '/Rainier1.png'\n",
    "img_2 = path + '/Rainier2.png'\n",
    "img_3 = path + '/Rainier3.png'\n",
    "img_4 = path + '/Rainier4.png'\n",
    "img_5 = path + '/Rainier5.png'\n",
    "img_6 = path + '/Rainier6.png'\n",
    "\n",
    "img_own_1 = path + '/1.jpg'\n",
    "img_own_2 = path + '/2.jpg'\n",
    "img_own_3 = path + '/3.jpg'\n",
    "\n",
    "img_extra_11 = path + '/Hanging1.png'\n",
    "img_extra_12 = path + '/Hanging2.png'\n",
    "\n",
    "img_extra_31 = path + '/ND1.png'\n",
    "img_extra_32 = path + '/ND2.png'\n",
    "\n",
    "img1 = cv2.imread(img_1)\n",
    "img2 = cv2.imread(img_2)\n",
    "img3 = cv2.imread(img_3)\n",
    "img4 = cv2.imread(img_4)\n",
    "img5 = cv2.imread(img_5)\n",
    "img6 = cv2.imread(img_6)\n",
    "\n",
    "own1 = cv2.imread(img_own_1)\n",
    "own2 = cv2.imread(img_own_2)\n",
    "own3 = cv2.imread(img_own_3)\n",
    "\n",
    "imgex11 = cv2.imread(img_extra_11)\n",
    "imgex12 = cv2.imread(img_extra_12)\n",
    "\n",
    "imgex31 = cv2.imread(img_extra_31)\n",
    "imgex32 = cv2.imread(img_extra_32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Implementation RANSAC and Stitching (Answer 3 and 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Compute the SIFTfeature descriptor using opencv Library. for a pair of images this takes as a parameter the two images and Returns the matches found and the keypoints for both the images (since for project it is not mandatory to use own implemntation, since my own implementation gives a correct output but takes a lot of time, so i preferred using opencv)\n",
    "\n",
    "#### function match: params(image 1 and image 2), returns number of matches and keypoint of both image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(img1,img2):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "    matcher = cv2.BFMatcher()\n",
    "    raw_matches = matcher.knnMatch(des1, des2, k=2)\n",
    "    good_matches=[]\n",
    "    good_points=[]\n",
    "    for m1, m2 in raw_matches:\n",
    "        if m1.distance < .4 * m2.distance:\n",
    "            good_points.append((m1.trainIdx, m1.queryIdx))\n",
    "            good_matches.append(m1)\n",
    "    return good_matches, kp1, kp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project points from image one to another image given a Homography, returs the projected image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(x1, y1, H):\n",
    "    w = (H[2][0]*x1 + H[2][1]*y1 + H[2][2])\n",
    "    if (w == 0):\n",
    "        w == 0.00000001\n",
    "    x2 = np.float32(((H[0][0]*x1 + H[0][1]*y1 + H[0][2])/w))\n",
    "    y2 = np.float32(((H[1][0]*x1 + H[1][1]*y1 + H[1][2])/w))\n",
    "    return x2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Inliers , threshold given\n",
    "computeInlierCount is a helper function for RANSAC that computes the number of inlying points given a homography \"H\". That is, project the first point in each match using the function \"project\". If the projected point is less than the distance \"inlierThreshold\" from the second point, it is an inlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeInlierCount(H, matches, inlierThreshold, keypoint1, keypoint2): \n",
    "    numMatches = 0\n",
    "    inliers=[]\n",
    "    for m in range(len(matches)):\n",
    "        x2, y2 = project(keypoint1[matches[m].queryIdx].pt[0], keypoint1[matches[m].queryIdx].pt[1], H)\n",
    "        x1 = keypoint2[matches[m].trainIdx].pt[0]\n",
    "        y1 = keypoint2[matches[m].trainIdx].pt[1]\n",
    "        distance = math.sqrt(((x1-x2)**2) + ((y1-y2)**2))\n",
    "        if(distance < inlierThreshold):\n",
    "            numMatches+= 1\n",
    "            inliers.append(matches[m])\n",
    "    return numMatches, inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANSAC Impelemntation \n",
    "<p> This function takes a list of potentially matching points between two images and returns the homography transformation that relates them </br>\n",
    "    <p> a. For \"numIterations\" iterations: \n",
    "        i. Randomly select 4 pairs of potentially matching points from \"matches\".</p>\n",
    "        ii. Compute the homography relating the four selected matches </p>\n",
    "        iii. If this homography produces the highest number of inliers, store it as the\n",
    "             best homography.</p></br>\n",
    "    <p> b. Given the highest scoring homography, once again find all the inliers. Compute a\n",
    "        new refined homography using all of the inliers (not just using four points as you\n",
    "        did previously. ) Compute an inverse homography as well, and return their values in\n",
    "        \"hom\" and \"homInv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RANSAC (matches, numIterations, inlierThreshold,  keypoint1, keypoint2):\n",
    "    maxNumInliers = 0\n",
    "    finalSrc =[]\n",
    "    finalDst =[]\n",
    "    for i in range(numIterations):\n",
    "        srcPoints =[]\n",
    "        dstPoints =[]\n",
    "        \n",
    "        four_random_matches = random.sample(matches, 4)\n",
    "#         print(four_random_matches//dmatch)\n",
    "        \n",
    "        for j in range(len(four_random_matches)):\n",
    "#             four_random_matches[i].trainIdx\n",
    "            srcPoints.append(keypoint1[four_random_matches[j].queryIdx].pt)\n",
    "            dstPoints.append(keypoint2[four_random_matches[j].trainIdx].pt)\n",
    "        src = np.float32(srcPoints).reshape(-1,1,2)\n",
    "        dst = np.float32(dstPoints).reshape(-1,1,2)\n",
    "        \n",
    "        H, status = cv2.findHomography(src, dst, 0)\n",
    "        if(H is None):\n",
    "            continue\n",
    "        \n",
    "        numMatches, inliers = computeInlierCount(H, matches, inlierThreshold, keypoint1, keypoint2)\n",
    "        if (numMatches > maxNumInliers):\n",
    "            maxNumInliers = numMatches\n",
    "            hom = H\n",
    "        \n",
    "    numMatchesFinal, inliersFinal = computeInlierCount(hom, matches, inlierThreshold,keypoint1, keypoint2)\n",
    "    \n",
    "    for k in range(len(inliersFinal)):\n",
    "        finalSrc.append(keypoint1[inliersFinal[k].queryIdx].pt)\n",
    "        finalDst.append(keypoint2[inliersFinal[k].trainIdx].pt)\n",
    "        \n",
    "    srcF = np.float32(finalSrc).reshape(-1,1,2)\n",
    "    dstF = np.float32(finalDst).reshape(-1,1,2)\n",
    "        \n",
    "    hom, status = cv2.findHomography(srcF, dstF,0)\n",
    "    homInv = np.float32(np.linalg.inv(hom))\n",
    "    \n",
    "    return hom, homInv,inliersFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stitch function returns the panorama and blendimage1 and blendImage2 (to be used in blending)\n",
    "Takes in the two images and the computed homography and inverse hoography . then computes the size of the stitched image given the two images then for each point of the image project\n",
    "Stitch the images together using the computed homography "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch(img1, img2, hom, homInv):\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    height, width = img2.shape[:2]\n",
    "    corners =[]\n",
    "    \n",
    "    corners.append(project(0,0, homInv))\n",
    "    corners.append(project(0, height - 1, homInv))\n",
    "    corners.append(project(width - 1, height - 1, homInv))\n",
    "    corners.append(project(width - 1, 0, homInv))\n",
    "    \n",
    "#     result = cv2.warpPerspective(img2, homInv,(img2.shape[1] + img1.shape[1], img2.shape[0]))\n",
    "#     cv2.imshow(\"Scanned Image\", result)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "    corners = np.float32( np.array(corners))\n",
    "    \n",
    "    (minX, maxX, minY, maxY )= (0, w1, 0, h1)\n",
    "    \n",
    "    for l in range(len(corners)):\n",
    "        if ((corners[l][0]) < minX):\n",
    "            minX = math.floor(corners[l][0])\n",
    "        elif((corners[l][0]) > maxX):\n",
    "            maxX = math.ceil(corners[l][0])\n",
    "            \n",
    "        if((corners[l][1]) < minY):\n",
    "            minY = math.floor(corners[l][1])\n",
    "        elif((corners[l][1]) > maxY):\n",
    "            maxY = math.ceil(corners[l][1])\n",
    "\n",
    "    minX = abs(minX)\n",
    "    minY = abs(minY)\n",
    "    \n",
    "    stitchedRow = (minY + maxY)\n",
    "    stitchedCol = (maxX + minX)\n",
    "\n",
    "    stitched = np.zeros([stitchedRow,stitchedCol, 3])\n",
    "    blendImg1 = np.zeros(stitched.shape)\n",
    "    blendImg2 = np.zeros(stitched.shape)\n",
    "    temp = np.zeros(stitched.shape)\n",
    "\n",
    "    \n",
    "    for h in range(h1):\n",
    "        for w in range(w1):\n",
    "            if img1[h][w][0] != 0 or img1[h][w][1] != 0 or img1[h][w][2] != 0:\n",
    "                stitched[h+minY][w + minX]  = img1[h][w]\n",
    "                temp[h+minY][w + minX]  = img1[h][w]\n",
    "                \n",
    "    for a in range(stitchedRow):\n",
    "        for b in range(stitchedCol):\n",
    "            xs , ys = project(b-minX, a-minY, hom)\n",
    "            xs = xs.astype(int)\n",
    "            ys = ys.astype(int)\n",
    "            if((temp[a][b]).all() !=0):\n",
    "                blendImg1[a][b] = temp[a][b]\n",
    "            \n",
    "            if(xs > 0 and xs < width and ys > 0 and ys < height):\n",
    "                stitched[a][b] = img2[ys][xs] \n",
    "                stitched[a][b] = cv2.getRectSubPix(img2, (1,1), (xs,ys)) \n",
    "                if((stitched[a][b]).all() !=0):\n",
    "                    blendImg2[a][b] = stitched[a][b] \n",
    "    \n",
    "    return stitched, blendImg1, blendImg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Ransac using two rainer image and save the final inlier (Answer 3)'\n",
    "at this point there is not a single wrong match. if there exists a wrong match the panorama will not be formed properly. \n",
    "NOTE: the number of inliers seems to be a bit higher because, the matching is calculated using slef implemntation while the inliers are calculated using cv.sift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_matches,kp1,kp2 = match(img1,img2)\n",
    "hom, homInv,inliersFinal  = RANSAC(good_matches,400,.6,kp1,kp2)\n",
    "image3 = cv2.drawMatches(img1,kp1, img2, kp2, inliersFinal, None, flags=2)\n",
    "cv2.imwrite(path_result+'/3'+'.png',image3.astype(np.uint8))\n",
    "cv2.imshow(\"Inliers\", image3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Stitched Image using two rainer image (Answer 4)\n",
    "Finally create a stiched image, \n",
    "#### also run all stitched and save the image with six rainer image (4a), \n",
    "take all the images in an array and call all the function inside a for loop, each step is saved for the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0\n",
    "pano = []\n",
    "panorama, blend1,blend2= stitch(img1, img2, hom, homInv)\n",
    "pano.append(panorama.astype(np.uint8))\n",
    "cv2.imwrite(path_result+'/4.png',panorama.astype(np.uint8))\n",
    "cv2.imshow(\"Stitched\", pano[0])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "imgs = [img1, img2, img3, img4, img5, img6]\n",
    "for m in range(len(imgs)):\n",
    "    if(m<(len(imgs)-2)):\n",
    "        good_matches,kp1,kp2 = match(pano[m],imgs[m+2])\n",
    "        hom, homInv,inliersFinal  = RANSAC(good_matches,400,.7,kp1,kp2)\n",
    "        panorama, b1,b2 = stitch(pano[m],imgs[m+2], hom, homInv)\n",
    "        pano.append(panorama.astype(np.uint8))\n",
    "        cv2.imwrite(path_result+'/steps_stitched%d.jpg' % (i),panorama.astype(np.uint8))\n",
    "        i+=1\n",
    "cv2.imwrite(path_result+'/Allstitched.png' ,pano[4].astype(np.uint8))\n",
    "cv2.imshow(\"AllStitched\", pano[4].astype(np.uint8))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use all the algorithm to stitch two images taken by myself\n",
    "//Takes a lot of time to run, sift.detectandcompute did not do a good job in creating the stitch. so used corner detection of harris and then used sift.compute to get the desciptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_own(img):\n",
    "    gray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    dst = cv2.cornerHarris(gray_img.astype(np.float32), 2, 3, 0.04)\n",
    "    result_img = img.copy() # deep copy image\n",
    "\n",
    "    # Threshold for an optimal value, it may vary depending on the image.\n",
    "    result_img[dst > 0.01 * dst.max()] = [0, 255, 0]\n",
    "\n",
    "    # for each dst larger than threshold, make a keypoint out of it\n",
    "    keypoints = np.argwhere(dst > 0.01 * dst.max())\n",
    "    keypoints = [cv2.KeyPoint(x[1], x[0], 1) for x in keypoints]\n",
    "\n",
    "    return (keypoints, result_img)\n",
    "\n",
    "def match_own(img1,img2, kp1, kp2):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    a, des1 = sift.compute(img1,kp1)\n",
    "    b, des2 = sift.compute(img2,kp2)\n",
    "    matcher = cv2.BFMatcher()\n",
    "    raw_matches = matcher.knnMatch(des1, des2, k=2)\n",
    "    good_points = []\n",
    "    good_matches=[]\n",
    "    for m1, m2 in raw_matches:\n",
    "        if m1.distance < .34 * m2.distance:\n",
    "            good_points.append(cv2.DMatch(m1.trainIdx, m1.queryIdx, 1))\n",
    "            good_matches.append(m1)\n",
    "    return good_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpo1 ,ds1= harris_own(own1)\n",
    "kpo2 ,ds2= harris_own(own2)\n",
    "kpo3 ,ds3= harris_own(own3)\n",
    "\n",
    "imgo1 = cv2.imread(img_own_1)\n",
    "imgo2 = cv2.imread(img_own_2)\n",
    "imgo3 = cv2.imread(img_own_3)\n",
    "\n",
    "good_matches_o = match_own(imgo1,imgo2, kpo1, kpo2)\n",
    "hom_o, homInv_o,inliersFinal_o  = RANSAC(good_matches_o,100,.2,kpo1,kpo2)\n",
    "pano_o, b1,b2= stitch(own1, own2, hom_o, homInv_o)\n",
    "cv2.imwrite(path_result+'/ownstep2.jpg',pano_o.astype(np.uint8))\n",
    "\n",
    "a = cv2.imread(path_result + '/ownstep2.jpg')\n",
    "kpos ,ds4 = harris_own(a)\n",
    "own = cv2.imread(path_result + '/ownstep2.jpg')\n",
    "good_matches_o2 = match_own(own ,imgo3, kpos, kpo3)\n",
    "hom_o2, homInv_o2,inliersFinal_o2  = RANSAC(good_matches_o2,300,.2,kpos,kpo3)\n",
    "pano_o2, b1,b2= stitch(own, own3, hom_o2, homInv_o2)\n",
    "cv2.imwrite(path_result+'/ownStitched.png',pano_o2.astype(np.uint8))\n",
    "\n",
    "cv2.imshow(\"ownStitched\", pano_o2.astype(np.uint8))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit (#4)\n",
    "### Image Blending \n",
    "take the warped image for the stich function output and belnd the images using laplacian pyramid building. I followed the procedure of thoery, taking the gaussian, then down sampling then subtracting the previous layer output for the next. then creating a laplacian filter and then blending them. For beautifying the top 64px of the stitched image is cropped since that portion was black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blending(img1, img2):\n",
    "    levels = 6\n",
    "    gaussianImg1 = [img1.astype(np.float32)]\n",
    "    gaussianImg2 = [img2.astype(np.float32)]\n",
    "    for i in range(levels):\n",
    "        img1 = cv2.pyrDown(img1).astype(np.float32)\n",
    "        gaussianImg1.append(img1)\n",
    "        img2 = cv2.pyrDown(img2).astype(np.float32)\n",
    "        gaussianImg2.append(img2)\n",
    "    laplacianImg1 = [gaussianImg1[levels]]\n",
    "    laplacianImg2 = [gaussianImg2[levels]]\n",
    "\n",
    "    for i in range(levels,0,-1):\n",
    "        temp = cv2.pyrUp(gaussianImg1[i]).astype(np.float32)\n",
    "        temp = cv2.resize(temp, (gaussianImg1[i-1].shape[1],gaussianImg1[i-1].shape[0]))\n",
    "        laplacianImg1.append(gaussianImg1[i-1]-temp)\n",
    "\n",
    "        temp = cv2.pyrUp(gaussianImg2[i]).astype(np.float32)\n",
    "        temp = cv2.resize(temp, (gaussianImg2[i-1].shape[1],gaussianImg2[i-1].shape[0]))\n",
    "        laplacianImg2.append(gaussianImg2[i-1]-temp)\n",
    "\n",
    "    laplacianList = []\n",
    "    for lpImg1,lpImg2 in zip(laplacianImg1,laplacianImg2):\n",
    "        rows,cols = lpImg1.shape[:2]\n",
    "        mask1 = np.zeros(lpImg1.shape,dtype = np.float32)\n",
    "        mask2 = np.zeros(lpImg2.shape,dtype = np.float32)\n",
    "        mask1[:, 0:int(cols/ 2)] = 1\n",
    "        mask2[:,int( cols / 2):] = 1\n",
    "\n",
    "        temp1 = lpImg1 * mask1\n",
    "        temp2 = lpImg2 * mask2\n",
    "        temp = temp1 + temp2\n",
    "        \n",
    "        laplacianList.append(temp)\n",
    "    \n",
    "    blend = laplacianList[0]\n",
    "    for i in range(1,levels+1):\n",
    "        blend = cv2.pyrUp(blend)   \n",
    "        blend = cv2.resize(blend, (laplacianList[i].shape[1],laplacianList[i].shape[0]))\n",
    "        blend = blend+ laplacianList[i]\n",
    "    \n",
    "    np.clip(blend, 0, 255, out=blend)\n",
    "    crop = blend[64:(64+len(blend)), 0:(0+len(blend[0]))]\n",
    "    cv2.imwrite(path_result+'/BlendedImage.png',crop.astype(np.uint8))\n",
    "    cv2.imshow(\"Blended\", crop.astype(np.uint8))\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "blending (blend1,blend2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA Credit ()\n",
    "\n",
    "A new keypoint detector implementing SIFT to detect the keypoints, used MOPS rotation invariance algorithm to describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotated image ransac, keypoint and sticthing\n",
    "the best result is found with 1000 iterations and inlierthreshold .09 and the ratio is .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key11, key12, good1, matched_keypoints1 = runDescriptor(imgex11, keypoints11, image11_magnitude, image11_orientations,\n",
    "              imgex12, keypoints12, image12_magnitude, image12_orientations, .9)\n",
    "good1, key11, key12 = match(imgex11,imgex12)\n",
    "cv2.imwrite(path_result+'/rotated_matching'+'.png',matched_keypoints1.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom1, homInv1,inliersFinal1  = RANSAC(good1,100,.09,key11,key12)\n",
    "image12 = cv2.drawMatches(imgex11,key11, imgex12, key12, inliersFinal1, None, flags=2)\n",
    "cv2.imwrite(path_result+'/rotated_matching_point_inliers'+'.png',image12.astype(np.uint8))\n",
    "cv2.imshow(\"rotated_matching_point_inliers\", image12)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "panorama1, _,_= stitch(imgex11, imgex12, hom1, homInv1)\n",
    "cv2.imwrite(path_result+'/rotated_stitched.png',panorama1.astype(np.uint8))\n",
    "cv2.imshow(\"rotated_stitched\", panorama1.astype(np.uint8))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD image ransac, keypoint and sticthing\n",
    "the best result is found with 600 iterations and inlierthreshold .6 and the ratio is .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key31, key32, good3, matched_keypoints3 = runDescriptor(imgex31, keypoints31, image31_magnitude, image31_orientations,\n",
    "              imgex32, keypoints32, image32_magnitude, image32_orientations, .75)\n",
    "cv2.imwrite(path_result+'/old_matching'+'.png',matched_keypoints3.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom3, homInv3,inliersFinal3  = RANSAC(good3,600,.7,key31,key32)\n",
    "image13 = cv2.drawMatches(imgex31,key31, imgex32, key32, inliersFinal3, None, flags=2)\n",
    "cv2.imwrite(path_result+'/old_matching_point_inliers'+'.png',image13.astype(np.uint8))\n",
    "cv2.imshow(\"old_matching_point_inliers\", image13)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with the same values of thrshold and iterations the Stitched image sometimes becomes bit different form each other,\n",
    "##issue can be solved by running the previous block again\n",
    "panorama3, _,_= stitch(imgex31, imgex32, hom3, homInv3)\n",
    "cv2.imwrite(path_result+'/old_stitched.png',panorama3.astype(np.uint8))\n",
    "cv2.imshow(\"old_stitched\", panorama3.astype(np.uint8))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# http://www.cs.utoronto.ca/~urtasun/courses/CV/lecture04.pdf\n",
    "# https://people.csail.mit.edu/hasinoff/320/sift-notes.txt\n",
    "# https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect6.pdf\n",
    "# https://github.com/geekyspartan/Image-stitching-panorama\n",
    "# https://github.com/agjayant/PanoramaStitching/blob/master/\n",
    "# https://www.csie.ntu.edu.tw/~cyy/courses/vfx/12spring/lectures/handouts/lec04_stitching_4up.pdf\n",
    "# https://docs.opencv.org/master/d1/de0/tutorial_py_feature_homography.html\n",
    "# https://courses.cs.washington.edu/courses/cse576/13sp/projects/project1/artifacts/tanvir/index.html\n",
    "# https://medium.com/@lerner98/implementing-sift-36c619df7945\n",
    "# https://kushalvyas.github.io/stitching.html\n",
    "# https://dsp.stackexchange.com/questions/4893/scale-and-rotation-invariant-feature-descriptors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
